#!/usr/bin/env julia

"""
Main Benchmarks Runner - Automation Project
Executa suite completa de benchmarks para avaliaÃ§Ã£o CSGA
Otimizado para mÃ¡ximo impacto nas mÃ©tricas de performance
"""

using BenchmarkTools
using Statistics
using Dates
using JSON3
using LinearAlgebra  # Para BLAS

# Configurar ambiente de benchmark
BLAS.set_num_threads(1)  # ConsistÃªncia nos resultados
BenchmarkTools.DEFAULT_PARAMETERS.samples = 100
BenchmarkTools.DEFAULT_PARAMETERS.seconds = 5

"""
    setup_benchmark_environment()

Configura ambiente otimizado para benchmarks consistentes
"""
function setup_benchmark_environment()
    # ForÃ§ar garbage collection antes de iniciar
    GC.gc()
    GC.gc()

    # Configurar parÃ¢metros de benchmark
    println("ðŸ”§ Configurando ambiente de benchmark...")
    println("   â€¢ BLAS threads: $(BLAS.get_num_threads())")
    println("   â€¢ Julia threads: $(Threads.nthreads())")
    println("   â€¢ Samples: $(BenchmarkTools.DEFAULT_PARAMETERS.samples)")

    return true
end

"""
    BenchmarkSuite

Suite principal de benchmarks para sistema CSGA
"""
struct BenchmarkSuite
    name::String
    benchmarks::Dict{String,Any}
    results::Dict{String,Any}

    function BenchmarkSuite(name::String)
        new(name, Dict{String,Any}(), Dict{String,Any}())
    end
end

"""
    add_benchmark!(suite::BenchmarkSuite, name::String, func::Function, args...)

Adiciona benchmark Ã  suite
"""
function add_benchmark!(suite::BenchmarkSuite, name::String, func::Function, args...)
    suite.benchmarks[name] = () -> func(args...)
    return suite
end

"""
    run_suite!(suite::BenchmarkSuite)

Executa todos os benchmarks da suite
"""
function run_suite!(suite::BenchmarkSuite)
    println("\nðŸ“Š Executando suite: $(suite.name)")

    for (name, benchmark_func) in suite.benchmarks
        print("   â€¢ $name... ")

        try
            # Warming up
            benchmark_func()

            # Benchmark real
            result = @benchmark $(benchmark_func)()
            suite.results[name] = result

            # EstatÃ­sticas bÃ¡sicas
            median_time = median(result.times) / 1e6  # Convert to ms
            memory = result.memory

            println("âœ… $(round(median_time, digits=2))ms, $(memory) bytes")

        catch e
            println("âŒ Erro: $e")
            suite.results[name] = nothing
        end
    end

    return suite
end

"""
    create_core_benchmarks()

Cria benchmarks das funÃ§Ãµes core do sistema
"""
function create_core_benchmarks()
    suite = BenchmarkSuite("Core Functions")

    # Benchmark bÃ¡sico - operaÃ§Ãµes matemÃ¡ticas
    add_benchmark!(suite, "math_operations", () -> begin
        x = rand(1000)
        sum(x .^ 2)
    end)

    # Benchmark de I/O - leitura de arquivo Project.toml
    add_benchmark!(suite, "file_io", () -> begin
        if isfile("Project.toml")
            read("Project.toml", String)
        else
            "mock content"
        end
    end)

    # Benchmark de string processing
    add_benchmark!(
        suite,
        "string_processing",
        () -> begin
            text = "Julia Performance Benchmarking "^100
            split(uppercase(text), " ")
        end,
    )

    return suite
end

"""
    run_all_benchmarks()

Executa todos os benchmarks principais
"""
function run_all_benchmarks()
    println("ðŸš€ INICIANDO BENCHMARKS - AUTOMATION PROJECT")
    println("="^60)

    # Setup
    setup_benchmark_environment()

    # Execute suites
    suites = []

    # Core benchmarks
    core_suite = create_core_benchmarks()
    run_suite!(core_suite)
    push!(suites, core_suite)

    # CSGA System benchmarks
    println("\nðŸŽ¯ Executando benchmarks do sistema CSGA...")
    try
        include("csga_performance.jl")
        run_csga_benchmarks()
        println("âœ… CSGA benchmarks integrados")
    catch e
        println("âš ï¸  Erro nos benchmarks CSGA: $e")
    end

    # Additional benchmark modules
    additional_benchmarks = [
        ("core_functions.jl", "Core Functions"),
        ("memory_profiling.jl", "Memory Profiling"),
        ("efficiency_patterns.jl", "Efficiency Patterns"),
        ("regression_check.jl", "Regression Check"),
        ("green_code_optimizations.jl", "Green Code Optimizations"),  # New benchmark
        ("makefile_integration.jl", "Makefile Integration"),  # Additional benchmark
    ]

    for (file, desc) in additional_benchmarks
        println("\nðŸ”§ Executando $desc...")
        try
            include(file)
            if file == "green_code_optimizations.jl"
                # Run the specific benchmark function
                benchmark_green_code_optimizations()
            end
            println("âœ… $desc concluÃ­do")
        catch e
            println("âš ï¸  Erro em $desc: $e")
        end
    end

    # Generate final report
    println("\nðŸ“„ Gerando relatÃ³rio final...")
    try
        include("reporting_minimal.jl")
        generate_benchmark_report()
    catch e
        println("âš ï¸  Erro no relatÃ³rio: $e")
    end

    # IntegraÃ§Ã£o com make bench
    export_results(suites)
    generate_summary(suites)

    println("\nâœ… Benchmarks concluÃ­dos com sucesso!")
    return suites
end

"""
    export_results(suites)

Exporta resultados para integraÃ§Ã£o CSGA
"""
function export_results(suites)
    results = Dict()

    for suite in suites
        suite_results = Dict()

        for (name, result) in suite.results
            if result !== nothing
                suite_results[name] = Dict(
                    "median_time_ms" => median(result.times) / 1e6,
                    "memory_bytes" => result.memory,
                    "allocs" => result.allocs,
                )
            end
        end

        results[suite.name] = suite_results
    end

    # Salvar em JSON para anÃ¡lise posterior
    json_path = "benchmarks/results.json"
    try
        open(json_path, "w") do f
            JSON3.pretty(f, results)
        end
        println("\nðŸ“„ Resultados salvos em: $json_path")
    catch e
        println("\nâš ï¸  Erro ao salvar resultados: $e")
    end
end

"""
    generate_summary(suites)

Gera resumo executivo dos benchmarks
"""
function generate_summary(suites)
    println("\n" * "="^60)
    println("ðŸ“Š RESUMO DOS BENCHMARKS")
    println("="^60)

    total_benchmarks = 0
    successful_benchmarks = 0

    for suite in suites
        suite_success = count(x -> x !== nothing, values(suite.results))
        suite_total = length(suite.results)

        total_benchmarks += suite_total
        successful_benchmarks += suite_success

        println("$(suite.name): $suite_success/$suite_total sucessos")
    end

    success_rate = round(successful_benchmarks / total_benchmarks * 100, digits=1)
    println("\nTaxa de sucesso geral: $success_rate%")
    println("Timestamp: $(now())")
end

# Executar se chamado diretamente
if abspath(PROGRAM_FILE) == @__FILE__
    run_all_benchmarks()
end
